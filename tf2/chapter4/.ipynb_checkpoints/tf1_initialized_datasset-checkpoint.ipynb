{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1_initialized_datasset\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl,np,sklearn,tf,keras:\n",
    "    print(module.__name__,module.__version__)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all,y_train_all),(x_test,y_test)=fashion_mnist.load_data()\n",
    "x_valid,x_train=x_train_all[:5000],x_train_all[5000:]\n",
    "y_valid,y_train=y_train_all[:5000],y_train_all[5000:]\n",
    "\n",
    "print(x_valid.shape,y_valid.shape)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(np.max(x_train),np.min(x_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 归一化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler =StandardScaler()\n",
    "# x_train: [None,28,28]-> [None,784]\n",
    "x_train_scaled=scaler.fit_transform(x_train.astype(np.float32).reshape(-1,1)).reshape(-1,28*28)\n",
    "x_valid_scaled=scaler.transform(x_valid.astype(np.float32).reshape(-1,1)).reshape(-1,28*28)\n",
    "x_test_scaled=scaler.transform(x_test.astype(np.float32).reshape(-1,1)).reshape(-1,28*28)\n",
    "\n",
    "y_train=np.asarray(y_train,dtype=np.int64)\n",
    "y_valid=np.asarray(y_valid,dtype=np.int64)\n",
    "y_test=np.asarray(y_test,dtype=np.int64)\n",
    "\n",
    "\n",
    "\n",
    "print(np.max(x_train_scaled),np.min(x_train_scaled))\n",
    "print(type(y_train),y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_dataset(images, labels, epochs ,bs , shuffle =True):\n",
    "    dataset= tf.data.Dataset.from_tensor_slices((images,labels))\n",
    "    if shuffle:\n",
    "        dataset=dataset.shuffle(10000)\n",
    "    dataset=dataset.repeat(epochs).batch(bs)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bs =20\n",
    "epochs=10\n",
    "images_placeholder=tf.placeholder(tf.float32,[None,28*28])\n",
    "labels_placeholder=tf.placeholder(tf.int64,[None,])\n",
    "\n",
    "dataset=make_dataset(images_placeholder,labels_placeholder,epochs=epochs,bs=bs)\n",
    "\n",
    "# 1 . auto initialization\n",
    "# 2. can't be re——intitialized. make_initializable_iterator\n",
    "\n",
    "dataset_iter=dataset.make_initializable_iterator()\n",
    "x,y=dataset_iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x_val,y_val=sess.run(dataset_iter.initializer,\n",
    "                        feed_dict={\n",
    "                            images_placeholder: x_train_scaled,\n",
    "                            labels_placeholder: y_train,})\n",
    "    x_val,y_val=sess.run([x,y])\n",
    "    print(x_val.shape)\n",
    "    print(y_val.shape)\n",
    "    \n",
    "    sess.run(data_iter.initializer,feed_dict={\n",
    "                                        images_placeholder:x_valid_scaled,labels_placeholder:y_valid,})\n",
    "    x_val,y_val=sess.run([x,y])\n",
    "    print(x_val.shape)\n",
    "    print(y_val.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_units=[100,100]\n",
    "class_num=10\n",
    "input_for_next_layer=x\n",
    "for hidden_unit in hidden_units:\n",
    "    input_for_next_layer=tf.layers.dense(input_for_next_layer,hidden_unit,activation=tf.nn.relu)\n",
    "\n",
    "logits=tf.layers.dense(input_for_next_layer,class_num)\n",
    "\n",
    "#  last_hidden_output*W(logits)-> softmax->prob\n",
    "#  1. logit -> softmax ->prob\n",
    "# 2. labels -> one_hot\n",
    "# 3. calculate cross entropy\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits)\n",
    "\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits,1)\n",
    "crrect_prediction=tf.equal(prediction,y)\n",
    "accuracy= tf.reduce_mean(tf.cast(crrect_prediction,tf.float64))\n",
    "\n",
    "train_op=tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x)\n",
    "print(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_steps_per_epoch=x_train.shape[0]//bs\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(train_steps_per_epoch):\n",
    "            loss_val, accuracy_val, _=sess.run([loss,accuracy,train_op])\n",
    "            print('\\r[Train] epoch: %d,step: %d, loss: %3.5f,accuracy: %2.2f'%(epoch,step,loss_val,accuracy_val),end=\"\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
