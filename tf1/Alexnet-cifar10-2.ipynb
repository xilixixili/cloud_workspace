{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import zipfile\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_activations(t):\n",
    "    print(t.op.name,'',t.get_shape().as_list)   #get_shape获取一个TensorShape对象，然后通过as_list方法返回每一个维度数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model():\n",
    "    _IMAGE_SIZE=32\n",
    "    _IMAGE_CHANNELS=3\n",
    "    _RESHAPE_SIZE=3*3*128\n",
    "    _NUM_CLASSES=10\n",
    " \n",
    "    parameters=[]\n",
    "    with  tf.name_scope('data'):\n",
    "        x=tf.placeholder(tf.float32,shape=[None,_IMAGE_SIZE*_IMAGE_SIZE*_IMAGE_CHANNELS],name='images')\n",
    "        y=tf.placeholder(tf.float32,shape=[None,_NUM_CLASSES],name='Output')\n",
    "        images=tf.reshape(x,[-1,_IMAGE_SIZE,_IMAGE_SIZE,_IMAGE_CHANNELS],name='images')\n",
    "        print(images) \n",
    "    #conv1\n",
    "    #这里name_scope实际上是为了解决共享变量的问题，在name_scope下进行tf.Variable(name)\n",
    "    #如果name重名，会自动检测命名冲突进行处理   \n",
    "    with tf.name_scope('conv1') as scope:          \n",
    "        kernel=tf.Variable(tf.truncated_normal([5,5,3,64],dtype=tf.float32,\n",
    "                                        stddev=1e-1),name='weights')\n",
    "        #变量解释 [a,b,c,d]分别表示,1表示是否跳过一些样本，比如a=1时，就是从1，2，3...训\n",
    "        #跳过一些，a=2时就选择1，3，5...，b表示高方向滑动，c表示宽方向滑动，d表示通道滑动\n",
    "        #same表示当卷积核超出边界时会进行0填充\n",
    "        conv=tf.nn.conv2d(images,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[64],dtype=tf.float32),\n",
    "                                        trainable=True,name='bias')\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv1=tf.nn.relu(bias,name=scope)    #这里返回的是一个tensor（一个张量类），但是这里的name=scope是什么意思？\n",
    "        print_activations(conv1)\n",
    "    tf.summary.histogram('Convolution_layers/conv1',conv1)\n",
    "    tf.summary.scalar('Convolution_layers/conver1',tf.nn.zero_fraction(conv1))\n",
    " \n",
    "    #这一步时local Response Normalization技术详情可以查看论文中描述\n",
    "    #lrn1\n",
    "    with tf.name_scope('lrn1') as scope:\n",
    "        lrn1=tf.nn.local_response_normalization(conv1,\n",
    "                                                alpha=1e-4,\n",
    "                                                beta=0.75,\n",
    "                                                depth_radius=2,\n",
    "                                                bias=2.0)\n",
    "    #pool1\n",
    "    pool1=tf.nn.max_pool(lrn1,ksize=[1,3,3,1],strides=[1,2,2,1], \n",
    "                    padding='VALID',name='pool1')\n",
    " \n",
    "    print_activations(pool1)\n",
    " \n",
    "    #conv2\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([5, 5, 64, 64], dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                                 trainable=True, name='biases')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope)\n",
    "    tf.summary.histogram('Convolution_layers/conv2',conv2)\n",
    "    tf.summary.scalar('Convolution_layers/conver2',tf.nn.zero_fraction(conv2))\n",
    "    print_activations(conv2)\n",
    "    #lrn2\n",
    "    with tf.name_scope('lrn2') as scope:\n",
    "        lrn2 = tf.nn.local_response_normalization(conv2,alpha=1e-4,beta=0.75,\n",
    "                                                depth_radius=2, bias=2.0)\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],\n",
    "                            padding='VALID',name='pool2')\n",
    "    print_activations(pool2)\n",
    " \n",
    "    #conv3\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        kernel =tf.Variable(tf.truncated_normal([3,3,64,128],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        conv=tf.nn.conv2d(pool2,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[128],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv3=tf.nn.relu(bias,name=scope)\n",
    "        print_activations(conv3)\n",
    "    tf.summary.histogram('Convolution_layers/conv3',conv3)\n",
    "    tf.summary.scalar('Convolution_layers/conver3',tf.nn.zero_fraction(conv3))\n",
    "    #conv4\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "        kernel =tf.Variable(tf.truncated_normal([3,3,128,128],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        conv=tf.nn.conv2d(conv3,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[128],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv4=tf.nn.relu(bias,name=scope)\n",
    "        print_activations(conv4)\n",
    "    tf.summary.histogram('Convolution_layers/conv4',conv4)\n",
    "    tf.summary.scalar('Convolution_layers/conver4',tf.nn.zero_fraction(conv4))\n",
    "    #conv5\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        kernel =tf.Variable(tf.truncated_normal([3,3,128,128],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        conv=tf.nn.conv2d(conv4,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[128],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        bias=tf.nn.bias_add(conv,biases)\n",
    "        conv5=tf.nn.relu(bias,name=scope)\n",
    "        print_activations(conv5)\n",
    "    tf.summary.histogram('Convolution_layers/conv5',conv5)\n",
    "    tf.summary.scalar('Convolution_layers/conver5',tf.nn.zero_fraction(conv5))\n",
    " \n",
    "    #pool5\n",
    "    pool5=tf.nn.max_pool(conv5,ksize=[1,3,3,1],strides=[1,2,2,1],\n",
    "                            padding='VALID',name='pool5')\n",
    "    print_activations(pool5)\n",
    " \n",
    "    #fully_connected1\n",
    "    with tf.name_scope('fully_connected1') as scope:\n",
    "        reshape=tf.reshape(pool5,[-1,_RESHAPE_SIZE])\n",
    "        dim=reshape.get_shape()[1].value\n",
    "        weights =tf.Variable(tf.truncated_normal([dim,384],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        print_activations(weights)\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[384],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        local3=tf.nn.relu(tf.matmul(reshape,weights)+biases,name=scope)\n",
    "        print_activations(local3)\n",
    "    tf.summary.histogram('Fully connected layers/fc1',local3)\n",
    "    tf.summary.scalar('Fully connected layers/fc1',tf.nn.zero_fraction(local3))\n",
    " \n",
    "    #fully_connected2\n",
    "    with tf.name_scope('fully_connected') as scope:\n",
    "        weights =tf.Variable(tf.truncated_normal([384,192],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        print_activations(weights)\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[192],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        local4=tf.nn.relu(tf.matmul(local3,weights)+biases,name=scope)\n",
    "        print_activations(local4)\n",
    "    tf.summary.histogram('Fully connected layers/fc2',local4)\n",
    "    tf.summary.scalar('Fully connected layers/fc4',tf.nn.zero_fraction(local4))\n",
    " \n",
    "    #output\n",
    "    with tf.name_scope('output') as scope:\n",
    "        weights =tf.Variable(tf.truncated_normal([192,_NUM_CLASSES],dtype=tf.float32,\n",
    "                                                stddev=1e-1),name='weights')\n",
    "        print_activations(weights)\n",
    "        biases=tf.Variable(tf.constant(0.0,shape=[_NUM_CLASSES],dtype=tf.float32),\n",
    "                                        trainable=True,name='biases')\n",
    "        softmax_linear=tf.add(tf.matmul(local4,weights),biases,name=scope)\n",
    "    tf.summary.histogram('Fully connected layers/output',softmax_linear)\n",
    " \n",
    "    global_step=tf.Variable(initial_value=0,name='global_step',trainable=False)\n",
    "    y_pred_cls=tf.argmax(softmax_linear,axis=1)\n",
    " \n",
    " \n",
    "    return x,y,softmax_linear,global_step,y_pred_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(name=\"train\", cifar=10):\n",
    "    x = None\n",
    "    y = None\n",
    "    l = None\n",
    " \n",
    "    maybe_download_and_extract()\n",
    " \n",
    "    folder_name = \"cifar_10\" if cifar == 10 else \"cifar_100\"\n",
    " \n",
    "    f = open('../data_set/'+folder_name+'/batches.meta', 'rb')\n",
    "    datadict = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    l = datadict['label_names']\n",
    " \n",
    "    if name is \"train\":\n",
    "        for i in range(5):\n",
    "            f = open('../data_set/'+folder_name+'/data_batch_' + str(i + 1), 'rb')\n",
    "            datadict = pickle.load(f, encoding='latin1')  #提取数据\n",
    "            f.close()\n",
    " \n",
    "            _X = datadict[\"data\"]\n",
    "            _Y = datadict['labels']\n",
    "            #print('_X')\n",
    "            #print(_X)\n",
    "            #print(np.shape(_X))\n",
    "            _X = np.array(_X, dtype=float) / 255.0\n",
    "            #print(np.shape(_X))\n",
    " \n",
    "            _X = _X.reshape([-1, 3, 32, 32])\n",
    "            #print(np.shape(_X))\n",
    "            _X = _X.transpose([0, 2, 3, 1])#矩阵转置,里面的编号是指将原来的维度变换到当前维度\n",
    "                                           #例如，原来的2变换到当前1维度\n",
    "            _X = _X.reshape(-1, 32*32*3)\n",
    "            if x is None:\n",
    "                x = _X\n",
    "                y = _Y\n",
    "            else:\n",
    "                x = np.concatenate((x, _X), axis=0)  #将x与读取的_X拼接起来\n",
    "                y = np.concatenate((y, _Y), axis=0)\n",
    " \n",
    "    elif name is \"test\":\n",
    "        f = open('../data_set/'+folder_name+'/test_batch', 'rb')\n",
    "        datadict = pickle.load(f, encoding='latin1')\n",
    "        f.close()\n",
    " \n",
    "        x = datadict[\"data\"]\n",
    "        y = np.array(datadict['labels'])\n",
    " \n",
    "        x = np.array(x, dtype=float) / 255.0\n",
    "        x = x.reshape([-1, 3, 32, 32])\n",
    "        x = x.transpose([0, 2, 3, 1])\n",
    "        x = x.reshape(-1, 32*32*3)\n",
    " \n",
    "    def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    " \n",
    "        return labels_one_hot\n",
    " \n",
    "    return x, dense_to_one_hot(y), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "def maybe_download_and_extract():\n",
    "    main_directory = \"../data_set/\"\n",
    "    cifar_10_directory = main_directory+\"cifar_10/\"\n",
    "    if not os.path.exists(main_directory):\n",
    "        os.makedirs(main_directory)\n",
    " \n",
    "        url = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "        filename = url.split('/')[-1]\n",
    "        file_path = os.path.join(main_directory, filename)\n",
    "        zip_cifar_10 = file_path\n",
    "        file_path, _ = urlretrieve(url=url, filename=file_path, reporthook=_print_download_progress)\n",
    " \n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(main_directory)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(main_directory)\n",
    "        print(\"Done.\")\n",
    " \n",
    "        os.rename(main_directory+\"./cifar-10-batches-py\", cifar_10_directory)\n",
    "        os.remove(zip_cifar_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Download progress: 5.8%"
     ]
    }
   ],
   "source": [
    "get_data_set(name=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
