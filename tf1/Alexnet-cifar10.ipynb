{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将传入的label转换成one hot的形式。\n",
    "def getOneHotLabel(label, depth):\n",
    "    m = np.zeros([len(label), depth])\n",
    "    for i in range(len(label)):\n",
    "        m[i][label[i]] = 1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立神经网络。\n",
    "def alexnet(image, keepprob=0.5):\n",
    "\n",
    "    # 定义卷积层1，卷积核大小，偏置量等各项参数参考下面的程序代码，下同。\n",
    "    with tf.name_scope(\"conv1\") as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 64], dtype=tf.float32, stddev=1e-1, name=\"weights\"))\n",
    "        conv = tf.nn.conv2d(image, kernel, [1, 4, 4, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[64]), trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope)\n",
    "\n",
    "        pass\n",
    "\n",
    "    # LRN层\n",
    "    lrn1 = tf.nn.lrn(conv1, 4, bias=1.0, alpha=0.001/9, beta=0.75, name=\"lrn1\")\n",
    "\n",
    "    # 最大池化层\n",
    "    pool1 = tf.nn.max_pool(lrn1, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"VALID\", name=\"pool1\")\n",
    "\n",
    "    # 定义卷积层2\n",
    "    with tf.name_scope(\"conv2\") as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([5,5,64,192], dtype=tf.float32, stddev=1e-1, name=\"weights\"))\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[192]), trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope)\n",
    "        pass\n",
    "\n",
    "    # LRN层\n",
    "    lrn2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9, beta=0.75, name=\"lrn2\")\n",
    "\n",
    "    # 最大池化层\n",
    "    pool2 = tf.nn.max_pool(lrn2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding=\"VALID\", name=\"pool2\")\n",
    "\n",
    "    # 定义卷积层3\n",
    "    with tf.name_scope(\"conv3\") as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,192,384], dtype=tf.float32, stddev=1e-1, name=\"weights\"))\n",
    "        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[384]), trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv3 = tf.nn.relu(bias, name=scope)\n",
    "        pass\n",
    "\n",
    "    # 定义卷积层4\n",
    "    with tf.name_scope(\"conv4\") as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,384,256], dtype=tf.float32, stddev=1e-1, name=\"weights\"))\n",
    "        conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[256]), trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv4 = tf.nn.relu(bias, name=scope)\n",
    "        pass\n",
    "\n",
    "    # 定义卷积层5\n",
    "    with tf.name_scope(\"conv5\") as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3,3,256,256], dtype=tf.float32, stddev=1e-1, name=\"weights\"))\n",
    "        conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "        biases = tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[256]), trainable=True, name=\"biases\")\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv5 = tf.nn.relu(bias, name=scope)\n",
    "        pass\n",
    "\n",
    "    # 最大池化层\n",
    "    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding=\"VALID\", name=\"pool5\")\n",
    "\n",
    "    # 全连接层\n",
    "    flatten = tf.reshape(pool5, [-1, 6*6*256])\n",
    "\n",
    "    weight1 = tf.Variable(tf.truncated_normal([6*6*256, 4096], mean=0, stddev=0.01))\n",
    "\n",
    "    fc1 = tf.nn.sigmoid(tf.matmul(flatten, weight1))\n",
    "\n",
    "    dropout1 = tf.nn.dropout(fc1, keepprob)\n",
    "\n",
    "    weight2 = tf.Variable(tf.truncated_normal([4096, 4096], mean=0, stddev=0.01))\n",
    "\n",
    "    fc2 = tf.nn.sigmoid(tf.matmul(dropout1, weight2))\n",
    "\n",
    "    dropout2 = tf.nn.dropout(fc2, keepprob)\n",
    "\n",
    "    weight3 = tf.Variable(tf.truncated_normal([4096, 10], mean=0, stddev=0.01))\n",
    "\n",
    "    fc3 = tf.nn.sigmoid(tf.matmul(dropout2, weight3))\n",
    "\n",
    "    return fc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_main():\n",
    "    # 加载使用的训练集文件名和标签。\n",
    "    files = np.load(\"label.npy\", encoding='bytes')[()]\n",
    "\n",
    "    # 提取文件名。\n",
    "    keys = [i for i in files]\n",
    "\n",
    "    print(len(keys))\n",
    "\n",
    "    myinput = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3], name='input')\n",
    "    mylabel = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='label')\n",
    "\n",
    "    # 建立网络，keepprob为0.6。\n",
    "    myoutput = alexnet(myinput, 0.6)\n",
    "\n",
    "    # 定义训练的loss函数。\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=myoutput, labels=mylabel))\n",
    "\n",
    "    # 定义优化器，学习率设置为0.09，学习率可以设置为其他的数值。\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.09).minimize(loss)\n",
    "\n",
    "    # 定义准确率\n",
    "    valaccuracy = tf.reduce_mean(\n",
    "        tf.cast(\n",
    "            tf.equal(\n",
    "                tf.argmax(myoutput, 1),\n",
    "                tf.argmax(mylabel, 1)),\n",
    "            tf.float32))\n",
    "\n",
    "    # tensorflow的saver，可以用于保存模型。\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # 40个epoch\n",
    "        for loop in range(40):\n",
    "\n",
    "            # 生成并打乱训练集的顺序。\n",
    "            indices = np.arange(50000)\n",
    "            random.shuffle(indices)\n",
    "\n",
    "            # batch size此处定义为200。\n",
    "            # 训练集一共50000张图片，前40000张用于训练，后10000张用于验证集。\n",
    "            for i in range(0, 0+40000, 200):\n",
    "                photo = []\n",
    "                label = []\n",
    "                for j in range(0, 200):\n",
    "                    # print(keys[indices[i + j]])\n",
    "                    photo.append(cv2.resize(cv2.imread(keys[indices[i + j]]), (224, 224))/225)\n",
    "                    label.append(files[keys[indices[i + j]]])\n",
    "                m = getOneHotLabel(label, depth=10)\n",
    "                a, b = sess.run([optimizer, loss], feed_dict={myinput: photo, mylabel: m})\n",
    "                print(\"\\r%lf\"%b, end='')\n",
    "\n",
    "            acc = 0\n",
    "            # 每次取验证集的200张图片进行验证，返回这200张图片的正确率。\n",
    "            for i in range(40000, 40000+10000, 200):\n",
    "                photo = []\n",
    "                label = []\n",
    "                for j in range(i, i + 200):\n",
    "                    photo.append(cv2.resize(cv2.imread(keys[indices[j]]), (224, 224))/225)\n",
    "                    label.append(files[keys[indices[j]]])\n",
    "                m = getOneHotLabel(label, depth=10)\n",
    "                acc += sess.run(valaccuracy, feed_dict={myinput: photo, mylabel: m})\n",
    "            # 输出，一共有50次验证集数据相加，所以需要除以50。\n",
    "            print(\"Epoch \", loop, ': validation rate: ', acc/50)\n",
    "        # 保存模型。\n",
    "        saver.save(sess, \"model/alex_cifar.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_test():\n",
    "    # 加载测试集的文件名和标签。\n",
    "    files = np.load(\"test-label.npy\", encoding='bytes')[()]\n",
    "    keys = [i for i in files]\n",
    "    print(len(keys))\n",
    "\n",
    "    myinput = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3], name='input')\n",
    "    mylabel = tf.placeholder(dtype=tf.float32, shape=[None, 10], name='label')\n",
    "    myoutput = alexnet(myinput, 0.6)\n",
    "\n",
    "    prediction = tf.argmax(myoutput, 1)\n",
    "    truth = tf.argmax(mylabel, 1)\n",
    "    valaccuracy = tf.reduce_mean(\n",
    "        tf.cast(\n",
    "            tf.equal(\n",
    "                prediction,\n",
    "                truth),\n",
    "            tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # 加载训练好的模型，路径根据自己的实际情况调整\n",
    "        saver.restore(sess, r\"model/model.ckpt\")\n",
    "\n",
    "        cnt = 0\n",
    "        for i in range(10000):\n",
    "            photo = []\n",
    "            label = []\n",
    "\n",
    "            photo.append(cv2.resize(cv2.imread(keys[i]), (224, 224))/225)\n",
    "            label.append(files[keys[i]])\n",
    "            m = getOneHotLabel(label, depth=10)\n",
    "            a, b= sess.run([prediction, truth], feed_dict={myinput: photo, mylabel: m})\n",
    "            print(a, ' ', b)\n",
    "            if a[0] == b[0]:\n",
    "                cnt += 1\n",
    "\n",
    "        print(\"Epoch \", 1, ': prediction rate: ', cnt / 10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
