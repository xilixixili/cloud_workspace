{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding:utf-8 -*-  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 10\n",
    "image_size = 32\n",
    "img_channels = 3\n",
    "iterations = 200\n",
    "batch_size = 250\n",
    "total_epoch = 164\n",
    "weight_decay = 0.0003\n",
    "dropout_rate = 0.5\n",
    "momentum_rate = 0.9\n",
    "log_save_path = './vgg_16_logs'\n",
    "model_save_path = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_data():\n",
    "    dirname = 'cifar10-dataset'\n",
    "    origin = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    fname = './CAFIR-10_data/cifar-10-python.tar.gz'\n",
    "    fpath = './' + dirname\n",
    "\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath) or os.path.isfile(fname):\n",
    "        download = False\n",
    "        print(\"DataSet already exist!\")\n",
    "    else:\n",
    "        download = True\n",
    "    if download:\n",
    "        print('Downloading data from', origin)\n",
    "        import urllib.request\n",
    "        import tarfile\n",
    "\n",
    "\n",
    "        def reporthook(count, block_size, total_size):\n",
    "            global start_time\n",
    "            if count == 0:\n",
    "                start_time = time.time()\n",
    "                return\n",
    "            duration = time.time() - start_time\n",
    "            progress_size = int(count * block_size)\n",
    "            speed = int(progress_size / (1024 * duration))\n",
    "            percent = min(int(count*block_size*100/total_size),100)\n",
    "            sys.stdout.write(\"\\r...%d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
    "                            (percent, progress_size / (1024 * 1024), speed, duration))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "        urllib.request.urlretrieve(origin, fname, reporthook)\n",
    "        print('Download finished. Start extract!', origin)\n",
    "        if fname.endswith(\"tar.gz\"):\n",
    "            tar = tarfile.open(fname, \"r:gz\")\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "        elif fname.endswith(\"tar\"):\n",
    "            tar = tarfile.open(fname, \"r:\")\n",
    "            tar.extractall()\n",
    "            tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_data_one(file):\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "    print(\"Loading %s : %d.\" % (file, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "    global image_size, img_channels\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n",
    "    data = data.reshape([-1, img_channels, image_size, image_size])\n",
    "    data = data.transpose([0, 2, 3, 1])\n",
    "    return data, labels\n",
    "\n",
    "def prepare_data():\n",
    "    print(\"======Loading data======\")\n",
    "    download_data()\n",
    "    data_dir = './cifar10-dataset'\n",
    "    image_dim = image_size * image_size * img_channels\n",
    "    meta = unpickle(data_dir + '/batches.meta')\n",
    "\n",
    "\n",
    "    print(meta)\n",
    "    label_names = meta[b'label_names']\n",
    "    label_count = len(label_names)\n",
    "    train_files = ['data_batch_%d' % d for d in range(1, 6)]\n",
    "    train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "    test_data, test_labels = load_data(['test_batch'], data_dir, label_count)\n",
    "\n",
    "\n",
    "    print(\"Train data:\", np.shape(train_data), np.shape(train_labels))\n",
    "    print(\"Test data :\", np.shape(test_data), np.shape(test_labels))\n",
    "    print(\"======Load finished======\")\n",
    "\n",
    "\n",
    "    print(\"======Shuffling data======\")\n",
    "    indices = np.random.permutation(len(train_data))\n",
    "    train_data = train_data[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "    print(\"======Prepare Finished======\")\n",
    "\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool(input, k_size=1, stride=1, name=None):\n",
    "    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1],\n",
    "                          padding='SAME', name=name)\n",
    "\n",
    "\n",
    "def batch_norm(input):\n",
    "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3,\n",
    "                                        is_training=train_flag, updates_collections=None)\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "    oshape = np.shape(batch[0])\n",
    "\n",
    "\n",
    "    if padding:\n",
    "        oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                      mode='constant', constant_values=0)\n",
    "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                    nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "def data_preprocessing(x_train,x_test):\n",
    "\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "\n",
    "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
    "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
    "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
    "\n",
    "\n",
    "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
    "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
    "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
    "\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    batch = _random_flip_leftright(batch)\n",
    "    batch = _random_crop(batch, [32, 32], 4)\n",
    "    return batch\n",
    "\n",
    "def learning_rate_schedule(epoch_num):\n",
    "    if epoch_num < 81:\n",
    "        return 0.1\n",
    "    elif epoch_num < 121:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_testing(sess, ep):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    pre_index = 0\n",
    "    add = 1000\n",
    "    for it in range(10):\n",
    "        batch_x = test_x[pre_index:pre_index+add]\n",
    "        batch_y = test_y[pre_index:pre_index+add]\n",
    "        pre_index = pre_index + add\n",
    "        loss_, acc_  = sess.run([cross_entropy, accuracy],\n",
    "                                feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0, train_flag: False})\n",
    "        loss += loss_ / 10.0\n",
    "        acc += acc_ / 10.0\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=loss), \n",
    "                                tf.Summary.Value(tag=\"test_accuracy\", simple_value=acc)])\n",
    "    return acc, loss, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    train_x, train_y, test_x, test_y = prepare_data()\n",
    "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
    "\n",
    "\n",
    "    # define placeholder x, y_ , keep_prob, learning_rate\n",
    "    x = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
    "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    train_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "    # build_network\n",
    "    W_conv1_1 = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_1 = bias_variable([64])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(x, W_conv1_1) + b_conv1_1))\n",
    "\n",
    "\n",
    "    W_conv1_2 = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_2 = bias_variable([64])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv1_2) + b_conv1_2))\n",
    "    output = max_pool(output, 2, 2, \"pool1\")\n",
    "\n",
    "\n",
    "    W_conv2_1 = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_1 = bias_variable([128])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv2_1) + b_conv2_1))\n",
    "\n",
    "\n",
    "    W_conv2_2 = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_2 = bias_variable([128])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv2_2) + b_conv2_2))\n",
    "    output = max_pool(output, 2, 2, \"pool2\")\n",
    "\n",
    "\n",
    "    W_conv3_1 = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_1 = bias_variable([256])\n",
    "    output = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1) + b_conv3_1))\n",
    "\n",
    "\n",
    "    W_conv3_2 = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_2 = bias_variable([256])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv3_2) + b_conv3_2))\n",
    "\n",
    "\n",
    "    W_conv3_3 = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_3 = bias_variable([256])\n",
    "    output = tf.nn.relu( batch_norm(conv2d(output, W_conv3_3) + b_conv3_3))\n",
    "    output = max_pool(output, 2, 2, \"pool3\")\n",
    "\n",
    "\n",
    "    W_conv4_1 = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_1 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv4_1) + b_conv4_1))\n",
    "\n",
    "\n",
    "    W_conv4_2 = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_2 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv4_2) + b_conv4_2))\n",
    "\n",
    "\n",
    "    W_conv4_3 = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_3 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv4_3) + b_conv4_3))\n",
    "    output = max_pool(output, 2, 2)\n",
    "\n",
    "\n",
    "    W_conv5_1 = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_1 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv5_1) + b_conv5_1))\n",
    "\n",
    "\n",
    "    W_conv5_2 = tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_2 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv5_2) + b_conv5_2))\n",
    "\n",
    "\n",
    "    W_conv5_3 = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_3 = bias_variable([512])\n",
    "    output = tf.nn.relu(batch_norm(conv2d(output, W_conv5_3) + b_conv5_3))\n",
    "    #output = max_pool(output, 2, 2)\n",
    "\n",
    "\n",
    "    # output = tf.contrib.layers.flatten(output)\n",
    "    output = tf.reshape(output, [-1, 2*2*512])\n",
    "\n",
    "\n",
    "    W_fc1 = tf.get_variable('fc1', shape=[2048, 4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc1 = bias_variable([4096])\n",
    "    output = tf.nn.relu(batch_norm(tf.matmul(output, W_fc1) + b_fc1) )\n",
    "    output = tf.nn.dropout(output, keep_prob)\n",
    "\n",
    "\n",
    "    W_fc2 = tf.get_variable('fc7', shape=[4096, 4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc2 = bias_variable([4096])\n",
    "    output = tf.nn.relu(batch_norm(tf.matmul(output, W_fc2) + b_fc2))\n",
    "    output = tf.nn.dropout(output, keep_prob)\n",
    "\n",
    "\n",
    "    W_fc3 = tf.get_variable('fc3', shape=[4096, 10], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc3 = bias_variable([10])\n",
    "    output = tf.nn.relu(batch_norm(tf.matmul(output, W_fc3) + b_fc3))\n",
    "\n",
    "\n",
    "    # output  = tf.reshape(output,[-1,10])\n",
    "\n",
    "\n",
    "    # loss function: cross_entropy\n",
    "    # train_step: training operation\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate, use_nesterov=True).\\\n",
    "        minimize(cross_entropy + l2 * weight_decay)\n",
    "\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    # initial an saver to save model\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
    "\n",
    "\n",
    "        # epoch = 164 \n",
    "        # make sure [bath_size * iteration = data_set_number]\n",
    "\n",
    "\n",
    "        for ep in range(1, total_epoch+1):\n",
    "            lr = learning_rate_schedule(ep)\n",
    "            pre_index = 0\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "            print(\"\\n epoch %d/%d:\" % (ep, total_epoch))\n",
    "\n",
    "\n",
    "            for it in range(1, iterations+1):\n",
    "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
    "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
    "\n",
    "\n",
    "                batch_x = data_augmentation(batch_x)\n",
    "\n",
    "\n",
    "                _, batch_loss = sess.run([train_step, cross_entropy],\n",
    "                                         feed_dict={x: batch_x, y_: batch_y, keep_prob: dropout_rate,\n",
    "                                                    learning_rate: lr, train_flag: True})\n",
    "                batch_acc = accuracy.eval(feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0, train_flag: True})\n",
    "\n",
    "\n",
    "                train_loss += batch_loss\n",
    "                train_acc += batch_acc\n",
    "                pre_index += batch_size\n",
    "\n",
    "\n",
    "                if it == iterations:\n",
    "                    train_loss /= iterations\n",
    "                    train_acc /= iterations\n",
    "\n",
    "\n",
    "                    loss_, acc_ = sess.run([cross_entropy, accuracy],\n",
    "                                           feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0, train_flag: True})\n",
    "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
    "                                               tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
    "\n",
    "\n",
    "                    val_acc, val_loss, test_summary = run_testing(sess, ep)\n",
    "\n",
    "\n",
    "                    summary_writer.add_summary(train_summary, ep)\n",
    "                    summary_writer.add_summary(test_summary, ep)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "\n",
    "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, \"\n",
    "                          \"train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\"\n",
    "                          % (it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
    "                else:\n",
    "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\"\n",
    "                          % (it, iterations, train_loss / it, train_acc / it))\n",
    "\n",
    "\n",
    "        save_path = saver.save(sess, model_save_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
